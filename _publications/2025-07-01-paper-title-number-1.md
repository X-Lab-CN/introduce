---
title: "5.PAST: Pairwise attention swin transformer for offline signature verification"
collection: publications
category: 2025
permalink: /publication/2025-07-01-paper-title-number-1
excerpt: '<div style="text-align: justify;">This paper addresses challenges in signature verification, proposing a Pairwise Attention mechanism to enable bidirectional info exchange between reference and query signatures without extra temporal assumptions. Combined with Swin Transformer, it forms PAST, resolving input fusion issues and performing well on datasets. It also finds training background info in CEDAR impacts results significantly.</div>'
date: 2025-07-01
venue: 'International Journal on Document Analysis and Recognition'
paperurl: 'http://xiongyujie.cn/files/PAST Pairwise attention swin transformer for offline signature verification.pdf'
citation: '<br/><div style="text-align: justify;">PAST: Pairwise attention swin transformer for offline signature verification, Y.-J. Xiong*, J.-X. Ren, D.-H. Zhu, X.-J. Xie, X.-H. Qiu, International Journal on Document Analysis and Recognition (IJDAR), 2025:1-13</div>'
---

<div style="text-align: justify;">Signature verification has shown tremendous potential as a reliable biometric in both academic research and industrial applications. With the advent of deep learning, signature verification has made remarkable progress in the past decade. However, despite significant progress, detecting subtle differences between genuine and forged signatures remains a challenge, raising concerns over privacy protection and data security in signature verification systems. Recently, the tremendous success of transformers in Natural Language Processing has led to their extension to computer vision, resulting in significant advancements. The multi-head self-attention mechanism is considered crucial for the success of Transformer. As the name implies, its query, key, and value all originate from the same sequence, rendering it suitable for single input tasks. However, pairwise signature verification treats reference and query signature images equally as two independent inputs. Regarding this matter, the mere amalgamation of the two independent inputs in the form of a single sequence inevitably leads to potential inherent issues. To tackle this problem, we present a Pairwise Attention (PA) mechanism that keeps the symmetry of inputs. Unlike the original attention mechanism, pairwise attention facilitates bidirectional information exchange between reference and query signatures without introducing any additional assumptive temporal information. Subsequently, combining with the architecture of Swin Transformer, we propose Pairwise Attention Swin Transformer(PAST). Our method fundamentally solves the problem of introducing false assumptive temporal information during the process of input fusion, and also performs impressively on several public datasets. Experimental results show that PAST outperforms most existing methods. In addition, we also investigated the impact of background information from the CEDAR database on the results. It revealed that background information in the training data has a significant impact on the verification performance.</div>

<br/>

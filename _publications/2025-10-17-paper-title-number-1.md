---
title: "7.MSA²: Multi-task Framework with Structure-aware and Style-adaptive Character Representation"
collection: publications
category: 2025
permalink: /publication/2025-10-17-paper-title-number-1
excerpt: '<div style="text-align: justify;">MoB is a training-free visual token pruning method for MLLMs that uses geometric covering theory to optimally balance prompt alignment and visual preservation, achieving high acceleration with minimal performance loss.</div>'
date: 2025-10-17
venue: 'Proceedings of the IEEE/CVF international conference on computer vision'
paperurl: 'http://xiongyujie.cn/files/1096_MSA_2_Multi_task_Framewor.pdf'
citation: '<br/><div style="text-align: justify;">MSA²: Multi-task Framework with Structure-aware and Style-adaptive Character Representation, Y.-F. Li, H.-J. Zhan*, Q. Liu, L. Sun, Y.-J. Xiong, Y. L, Proceedings of the IEEE/CVF international conference on computer vision, 2025(accepted)</div>'
---

<div style="text-align: justify;">Most existing methods regard open-set Chinese text recognition (CTR) as a single-task problem, primarily focusing on prototype learning of linguistic components or glyphs to identify unseen characters. In contrast, humans identify characters by integrating multiple perspectives, including linguistic and visual cues. Inspired by this, we propose a multi-task framework termed MSA, which considers multi-view character representations for open-set CTR. Within MSA, we introduce two novel strategies for character representation: structure-aware component encoding (SACE) and style-adaptive glyph embedding (SAGE). SACE utilizes a binary tree with dynamic representation space to emphasize the primary linguistic components, thereby generating structure-aware and discriminative linguistic representations for each character. Meanwhile, SAGE employs a glyph-centric contrastive learning to aggregate features from diverse forms, yielding robust glyph representations for the CTR model to adapt to the style variations among various fonts. Extensive experiments demonstrate that our proposed MSA outperforms state-of-the-art CTR methods, achieving an average improvement of 1.3% and 6.0% in accuracy under closed-set and open-set settings on the BCTR dataset, respectively. The code will be available soon.</div>

<br/>

